# ローカル LLM

作成日 2025/03/06

## 1. VSCode の機能拡張に Continue あり

マーケットプレース => [Continue - Codestral, Claude, and more](https://marketplace.visualstudio.com/items?itemName=Continue.continue)

公式ドキュメント（英語） => [Introduction | Continue](https://docs.continue.dev/)

## 2. OpenAI API と互換性のあるローカルサーバーを立ち上げる

参照サイト 1 => [機密情報も安心？ローカル実行可能な LLM で vscode 開発環境を作る](https://qiita.com/kota33/items/63ba76dee2535374af0d)

参照サイト 2 => [ローカル LLM を VSCode で Cursor のように使える continue.dev の設定方法](https://note.com/shinao39/n/nc8b580b80f15)

### 2a. llama.cpp

ローカルで LLM を実行する際に最も人気のあるツール

ソースコード => [ggml-org/llama.cpp: LLM inference in C/C++](https://github.com/ggml-org/llama.cpp)
