# モデルの違い

作成日 2025/04/22

公開されているLLMには、いくつか違うモデルが用意されている

- pt ... pretained 事前学習済みモデル
- it ... instruction-tuned 命令チューニング済みモデル

このモデルの違いとは何か？

[What is the difference between pre-training, fine-tuning, and instruct-tuning exactly?](https://www.reddit.com/r/learnmachinelearning/comments/19f04y3/what_is_the_difference_between_pretraining/)

> **Pre-training** is the initial phase of training an LLM where it learns from a large, diverse dataset of often trillions of tokens. The goal here is to develop a broad understanding of language, context, and various types of knowledge. Pre-training is usually MASSIVELY computationally expensive and requires HUGE amounts of data. We're often talking in the millions of dollars when pre-training models. This is one of the primary reasons I am personally skeptical of the idea of open source overtaking commercial models, since most open-weight LLMs still had to come from a corporation making the initial investment in pre-training. Suffice it to say as a beginner, you will not be doing any pre-training.
>
> **Instruct-tuning** is a relatively newer concept, used in models like ChatGPT and InstructGPT, where the model is trained (or further trained) to follow instructions in prompts better. This doesn't necessarily involve adding new factual knowledge to the model; rather, it's about training the model to understand and execute given instructions more effectively. It's more about improving the model's ability to parse and respond to prompts in a way that aligns with user intentions.

理解したこと => ptは事前学習が終わった直後のモデル。初心者がptに触る必要はない。itはプロンプトによる命令に従うように訓練された後のモデル
